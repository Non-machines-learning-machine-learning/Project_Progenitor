*** Natural Language Toolkit, NLTK

* Tokenization 
The process of demarcating and possibly classifying sections of a string of input characters. The resulting tokens are then passed on to some other form of processing.

* Sentence boundary disambiguation (SBD)
Also known as sentence breaking, is the problem in natural language processing of deciding where sentences begin and end.
Sent_tokenize uses an instance of PunktSentenceTokenizer from the nltk. tokenize.punkt module. 
This instance has already been trained on and works well for many European languages. 
So it knows what punctuation and characters mark the end of a sentence and the beginning of a new sentence.

    from nltk.tokenize import sent_tokenize
    sent_tokenize_list = sent_tokenize(text)

* Word Tokenizing
    from nltk.tokenize import word_tokenize
    word_tokenize(‘Hello World.’)

    This tokenizer is designed to work on a sentence at a time.

* Part-of-speech (POS) tagging
process of marking up a word in a text (corpus) as corresponding to a particular part of speech, based on both its definition, 
as well as its context—i.e. relationship with adjacent and related words in a phrase, sentence, or paragraph.

    import nltk
    text = nltk.word_tokenize(“Dive into NLTK: Part-of-speech tagging and POS Tagger”)
    tags = nltk.pos_tag(text)   #list of tuples, (word,tag)

    NLTK provides documentation for each tag, which can be queried using the tag, e.g., 
    nltk.help.upenn_tagset(‘RB’),

* Stemming 
Process for reducing inflected (or sometimes derived) words to their stem, base or root form—generally a written word form.
NLTK provides several famous stemmers interfaces, such as Porter stemmer, Lancaster Stemmer, Snowball Stemmer and etc.

    from nltk.stem.porter import PorterStemmer
    porter_stemmer = PorterStemmer()
    porter_stemmer.stem(‘presumably’)

* Lemmatization
Process of grouping together the different inflected forms of a word so they can be analysed as a single item.
lemmatisation is the algorithmic process of determining the lemma for a given word. 
Since the process may involve complex tasks such as understanding context and determining the part of speech of a word in a sentence 
(requiring, for example, knowledge of the grammar of a language) it can be a hard task to implement a lemmatiser for a new language.

Lemmatisation is closely related to stemming. The difference is that a stemmer operates on a single word without knowledge of the context, 
and therefore cannot discriminate between words which have different meanings depending on part of speech. However, 
stemmers are typically easier to implement and run faster, and the reduced accuracy may not matter for some applications.

*** Scikit-Learn

Text data needs to be tokenized before it can be used for predicitve modelling. 
This processes parses text into words that are then encoded (Tokenizated) as integers or floating point values for use as inputs into machine
learning algorithms (where feature extraction can occur). 

Vectorization 
    is the term for converting a scalar program to a vector program. Vectorized programs can run multiple operations 
    from a single instruction, whereas scalar can only operate on pairs of operands at once.

Bag-of-Words Model, or BoW
    method of document classification where the (frequency of) occurrence of each word is used as a feature for training a classifier.
     concerned with encoding schemes that represent what words are present or the degree to which they are present in 
     encoded documents without any information about order.





